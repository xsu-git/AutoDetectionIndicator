#!/usr/bin/env python
# -*- coding: utf-8 -*-
'''
@Time              @Author    @Version    @Desciption
---------------    -------    --------    -----------
2025/9/9 13:07     Xsu         1.0         None
'''

# !/usr/bin/env python
# -*- coding: utf-8 -*-
"""
å³°å€¼æ„ŸçŸ¥çš„æœºå™¨å­¦ä¹ æ¨¡å‹
ä¸“é—¨é’ˆå¯¹å³°å€¼ç‚¹ç‰¹å¾å­¦ä¹ ä¼˜åŒ–
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import (
    RandomForestClassifier,
    GradientBoostingClassifier,
    IsolationForest,
    VotingClassifier
)
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import TimeSeriesSplit, cross_val_score
from sklearn.metrics import precision_recall_curve, roc_auc_score
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline as ImbPipeline
import xgboost as xgb
import lightgbm as lgb
from typing import Dict, Tuple, List
import warnings

warnings.filterwarnings('ignore')


class PeakAwareMLModel:
    """
    å³°å€¼æ„ŸçŸ¥çš„æœºå™¨å­¦ä¹ æ¨¡å‹
    - ä¸“æ³¨äºå³°å€¼ç‚¹çš„ç‰¹å¾å­¦ä¹ 
    - å¤„ç†æ ·æœ¬ä¸å¹³è¡¡
    - å¤šæ¨¡å‹é›†æˆ
    """

    def __init__(self):
        self.models = {}
        self.scalers = {}
        self.feature_importance = {}
        self.peak_patterns = {}

    def prepare_peak_focused_data(self, df_tech: pd.DataFrame, features_df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict]:
        """
        å‡†å¤‡å³°å€¼èšç„¦çš„è®­ç»ƒæ•°æ®
        """
        print("ğŸ¯ å‡†å¤‡å³°å€¼èšç„¦æ•°æ®...")

        # 1. è¯†åˆ«å³°å€¼ç‚¹
        peak_mask = df_tech['is_peak'] == 1
        high_peak_mask = df_tech['peak_type'] == 1
        low_peak_mask = df_tech['peak_type'] == -1

        # 2. åˆ›å»ºå³°å€¼ç‰¹å®šçš„ç›®æ ‡å˜é‡
        targets = {}

        # å³°å€¼åçš„åè½¬æ¦‚ç‡ï¼ˆæ›´åˆç†çš„ç›®æ ‡ï¼‰
        targets['peak_reversal_3'] = self._calculate_reversal_probability(df_tech, 3)
        targets['peak_reversal_5'] = self._calculate_reversal_probability(df_tech, 5)

        # å³°å€¼è´¨é‡è¯„åˆ†ï¼ˆåŸºäºå†å²è¡¨ç°ï¼‰
        targets['peak_quality'] = self._calculate_peak_quality(df_tech, features_df)

        # å³°å€¼åçš„è¶‹åŠ¿æŒç»­æ€§
        targets['trend_continuation'] = self._calculate_trend_continuation(df_tech)

        # 3. æ„é€ å³°å€¼ç‰¹å®šç‰¹å¾
        peak_features = self._engineer_peak_specific_features(df_tech, features_df)

        # 4. æ ·æœ¬å¹³è¡¡ç­–ç•¥
        balanced_data = self._balance_peak_samples(peak_features, targets, peak_mask)

        return balanced_data, targets

    def _calculate_reversal_probability(self, df_tech: pd.DataFrame, period: int) -> pd.Series:
        """
        è®¡ç®—å³°å€¼åçš„åè½¬æ¦‚ç‡
        é«˜ç‚¹åä¸‹è·Œã€ä½ç‚¹åä¸Šæ¶¨çš„æ¦‚ç‡
        """
        reversal = pd.Series(0, index=df_tech.index)

        for i in range(len(df_tech) - period):
            if df_tech['peak_type'].iloc[i] == 1:  # é«˜ç‚¹
                # æ£€æŸ¥åç»­æ˜¯å¦ä¸‹è·Œ
                future_return = (df_tech['close'].iloc[i + period] - df_tech['close'].iloc[i]) / df_tech['close'].iloc[
                    i]
                reversal.iloc[i] = 1 if future_return < -0.01 else 0  # ä¸‹è·Œè¶…è¿‡1%

            elif df_tech['peak_type'].iloc[i] == -1:  # ä½ç‚¹
                # æ£€æŸ¥åç»­æ˜¯å¦ä¸Šæ¶¨
                future_return = (df_tech['close'].iloc[i + period] - df_tech['close'].iloc[i]) / df_tech['close'].iloc[
                    i]
                reversal.iloc[i] = 1 if future_return > 0.01 else 0  # ä¸Šæ¶¨è¶…è¿‡1%

        return reversal

    def _calculate_peak_quality(self, df_tech: pd.DataFrame, features_df: pd.DataFrame) -> pd.Series:
        """
        è®¡ç®—å³°å€¼è´¨é‡åˆ†æ•°
        åŸºäºå¤šä¸ªç»´åº¦è¯„ä¼°å³°å€¼çš„é‡è¦æ€§
        """
        quality_score = pd.Series(0.0, index=df_tech.index)

        # 1. ä»·æ ¼çªç ´ç¨‹åº¦
        price_breakout = abs(df_tech['close'] - df_tech['sma_20']) / df_tech['sma_20']

        # 2. æˆäº¤é‡å¼‚å¸¸
        if 'volume' in df_tech.columns:
            vol_ratio = df_tech['volume'] / df_tech['volume'].rolling(20).mean()
        else:
            vol_ratio = 1.0

        # 3. æŠ€æœ¯æŒ‡æ ‡æå€¼
        rsi_extreme = np.where(df_tech['rsi_14'] > 70, (df_tech['rsi_14'] - 70) / 30,
                               np.where(df_tech['rsi_14'] < 30, (30 - df_tech['rsi_14']) / 30, 0))

        # 4. æ³¢åŠ¨ç‡çŠ¶æ€
        atr_percentile = df_tech['atr_14'].rolling(50).rank(pct=True)

        # ç»¼åˆè¯„åˆ†
        quality_score = (
                price_breakout * 0.3 +
                np.clip(vol_ratio, 0, 2) / 2 * 0.3 +
                rsi_extreme * 0.2 +
                atr_percentile * 0.2
        )

        # åªä¿ç•™å³°å€¼ç‚¹çš„åˆ†æ•°
        quality_score[df_tech['is_peak'] == 0] = 0

        # åˆ†ç±»ï¼šé«˜è´¨é‡(>0.7)ã€ä¸­ç­‰(0.4-0.7)ã€ä½è´¨é‡(<0.4)
        return pd.cut(quality_score, bins=[0, 0.4, 0.7, 1.0], labels=[0, 1, 2])

    def _calculate_trend_continuation(self, df_tech: pd.DataFrame) -> pd.Series:
        """
        è®¡ç®—è¶‹åŠ¿å»¶ç»­æ€§
        å³°å€¼åè¶‹åŠ¿æ˜¯å¦ç»§ç»­
        """
        continuation = pd.Series(0, index=df_tech.index)

        for i in range(20, len(df_tech) - 10):
            if df_tech['is_peak'].iloc[i] == 1:
                # å³°å€¼å‰çš„è¶‹åŠ¿
                pre_trend = np.polyfit(range(10), df_tech['close'].iloc[i - 10:i].values, 1)[0]
                # å³°å€¼åçš„è¶‹åŠ¿
                post_trend = np.polyfit(range(10), df_tech['close'].iloc[i:i + 10].values, 1)[0]

                # åŒå‘ä¸ºå»¶ç»­ï¼Œåå‘ä¸ºåè½¬
                continuation.iloc[i] = 1 if pre_trend * post_trend > 0 else -1

        return continuation

    def _engineer_peak_specific_features(self, df_tech: pd.DataFrame, features_df: pd.DataFrame) -> pd.DataFrame:
        """
        æ„é€ å³°å€¼ç‰¹å®šç‰¹å¾
        """
        peak_features = features_df.copy()

        # 1. å³°å€¼é—´è·ç‰¹å¾
        peak_features['bars_between_peaks'] = self._calculate_peak_spacing(df_tech)

        # 2. å³°å€¼ç›¸å¯¹å¼ºåº¦
        peak_features['relative_peak_height'] = self._calculate_relative_height(df_tech)

        # 3. å³°å€¼å½¢æ€ç‰¹å¾
        peak_features['peak_sharpness'] = self._calculate_peak_sharpness(df_tech)
        peak_features['peak_symmetry'] = self._calculate_peak_symmetry(df_tech)

        # 4. å¤šæ—¶é—´æ¡†æ¶ç¡®è®¤
        for tf in [5, 10, 20]:
            peak_features[f'mtf_alignment_{tf}'] = self._check_multi_timeframe_alignment(df_tech, tf)

        # 5. å³°å€¼èšé›†åº¦
        peak_features['peak_cluster_density'] = self._calculate_peak_clustering(df_tech)

        # 6. åŠ¨é‡èƒŒç¦»ç‰¹å¾
        peak_features['momentum_divergence'] = self._calculate_momentum_divergence(df_tech)

        # 7. æ”¯æ’‘é˜»åŠ›ç‰¹å¾
        peak_features['sr_distance'] = self._calculate_support_resistance_distance(df_tech)

        return peak_features

    def _calculate_peak_spacing(self, df_tech: pd.DataFrame) -> pd.Series:
        """è®¡ç®—å³°å€¼é—´è·"""
        spacing = pd.Series(np.nan, index=df_tech.index)
        peak_indices = df_tech[df_tech['is_peak'] == 1].index

        for i in range(1, len(peak_indices)):
            current_idx = peak_indices[i]
            prev_idx = peak_indices[i - 1]
            spacing.loc[current_idx] = (current_idx - prev_idx).days if hasattr(current_idx - prev_idx, 'days') else i

        return spacing.fillna(method='ffill')

    def _calculate_relative_height(self, df_tech: pd.DataFrame) -> pd.Series:
        """è®¡ç®—ç›¸å¯¹å³°å€¼é«˜åº¦"""
        relative_height = pd.Series(0.0, index=df_tech.index)

        for i in range(10, len(df_tech) - 10):
            if df_tech['is_peak'].iloc[i] == 1:
                window_prices = df_tech['close'].iloc[i - 10:i + 11]

                if df_tech['peak_type'].iloc[i] == 1:  # é«˜ç‚¹
                    relative_height.iloc[i] = (df_tech['close'].iloc[i] - window_prices.min()) / window_prices.min()
                else:  # ä½ç‚¹
                    relative_height.iloc[i] = (window_prices.max() - df_tech['close'].iloc[i]) / df_tech['close'].iloc[
                        i]

        return relative_height

    def _calculate_peak_sharpness(self, df_tech: pd.DataFrame) -> pd.Series:
        """è®¡ç®—å³°å€¼å°–é”åº¦"""
        sharpness = pd.Series(0.0, index=df_tech.index)

        for i in range(2, len(df_tech) - 2):
            if df_tech['is_peak'].iloc[i] == 1:
                # è®¡ç®—å³°å€¼ç‚¹å‰åçš„æ–œç‡å˜åŒ–
                left_slope = (df_tech['close'].iloc[i] - df_tech['close'].iloc[i - 2]) / 2
                right_slope = (df_tech['close'].iloc[i + 2] - df_tech['close'].iloc[i]) / 2

                sharpness.iloc[i] = abs(left_slope - right_slope)

        return sharpness

    def _calculate_peak_symmetry(self, df_tech: pd.DataFrame) -> pd.Series:
        """è®¡ç®—å³°å€¼å¯¹ç§°æ€§"""
        symmetry = pd.Series(0.0, index=df_tech.index)

        for i in range(5, len(df_tech) - 5):
            if df_tech['is_peak'].iloc[i] == 1:
                left_profile = df_tech['close'].iloc[i - 5:i].values
                right_profile = df_tech['close'].iloc[i + 1:i + 6].values

                # å½’ä¸€åŒ–
                left_norm = (left_profile - left_profile.min()) / (left_profile.max() - left_profile.min() + 1e-8)
                right_norm = (right_profile - right_profile.min()) / (right_profile.max() - right_profile.min() + 1e-8)

                # è®¡ç®—å¯¹ç§°æ€§ï¼ˆç›¸ä¼¼åº¦ï¼‰
                symmetry.iloc[i] = 1 - np.mean(np.abs(left_norm - right_norm[::-1]))

        return symmetry

    def _check_multi_timeframe_alignment(self, df_tech: pd.DataFrame, timeframe: int) -> pd.Series:
        """æ£€æŸ¥å¤šæ—¶é—´æ¡†æ¶å¯¹é½"""
        alignment = pd.Series(0, index=df_tech.index)

        # è®¡ç®—æ›´å¤§æ—¶é—´æ¡†æ¶çš„ç§»åŠ¨å‡çº¿
        ma_tf = df_tech['close'].rolling(timeframe).mean()

        # æ£€æŸ¥å³°å€¼æ˜¯å¦ä¸å¤§æ—¶é—´æ¡†æ¶è¶‹åŠ¿ä¸€è‡´
        for i in range(timeframe, len(df_tech)):
            if df_tech['is_peak'].iloc[i] == 1:
                if df_tech['peak_type'].iloc[i] == 1:  # é«˜ç‚¹
                    # é«˜ç‚¹åº”è¯¥åœ¨ä¸Šå‡è¶‹åŠ¿ä¸­
                    alignment.iloc[i] = 1 if ma_tf.iloc[i] > ma_tf.iloc[i - timeframe] else -1
                else:  # ä½ç‚¹
                    # ä½ç‚¹åº”è¯¥åœ¨ä¸‹é™è¶‹åŠ¿ä¸­
                    alignment.iloc[i] = 1 if ma_tf.iloc[i] < ma_tf.iloc[i - timeframe] else -1

        return alignment

    def _calculate_peak_clustering(self, df_tech: pd.DataFrame) -> pd.Series:
        """è®¡ç®—å³°å€¼èšé›†åº¦"""
        clustering = pd.Series(0.0, index=df_tech.index)
        window = 20

        for i in range(window, len(df_tech)):
            window_data = df_tech.iloc[i - window:i + 1]
            peak_count = window_data['is_peak'].sum()
            clustering.iloc[i] = peak_count / window

        return clustering

    def _calculate_momentum_divergence(self, df_tech: pd.DataFrame) -> pd.Series:
        """è®¡ç®—åŠ¨é‡èƒŒç¦»"""
        divergence = pd.Series(0, index=df_tech.index)

        for i in range(14, len(df_tech)):
            if df_tech['is_peak'].iloc[i] == 1:
                # ä»·æ ¼è¶‹åŠ¿
                price_trend = np.polyfit(range(14), df_tech['close'].iloc[i - 13:i + 1].values, 1)[0]

                # RSIè¶‹åŠ¿
                rsi_trend = np.polyfit(range(14), df_tech['rsi_14'].iloc[i - 13:i + 1].values, 1)[0]

                # èƒŒç¦»æ£€æµ‹
                if df_tech['peak_type'].iloc[i] == 1:  # é«˜ç‚¹
                    # ä»·æ ¼åˆ›æ–°é«˜ä½†RSIæ²¡æœ‰ - çœ‹è·ŒèƒŒç¦»
                    divergence.iloc[i] = -1 if price_trend > 0 and rsi_trend < 0 else 0
                else:  # ä½ç‚¹
                    # ä»·æ ¼åˆ›æ–°ä½ä½†RSIæ²¡æœ‰ - çœ‹æ¶¨èƒŒç¦»
                    divergence.iloc[i] = 1 if price_trend < 0 and rsi_trend > 0 else 0

        return divergence

    def _calculate_support_resistance_distance(self, df_tech: pd.DataFrame) -> pd.Series:
        """è®¡ç®—åˆ°æ”¯æ’‘é˜»åŠ›çš„è·ç¦»"""
        distance = pd.Series(0.0, index=df_tech.index)

        for i in range(50, len(df_tech)):
            current_price = df_tech['close'].iloc[i]

            # æ‰¾å‡ºè¿‡å»50æ ¹Kçº¿çš„å³°å€¼ä½œä¸ºæ½œåœ¨æ”¯æ’‘é˜»åŠ›
            past_peaks = df_tech.iloc[i - 50:i][df_tech['is_peak'].iloc[i - 50:i] == 1]['close']

            if len(past_peaks) > 0:
                # æ‰¾æœ€è¿‘çš„æ”¯æ’‘å’Œé˜»åŠ›
                resistance = past_peaks[past_peaks > current_price]
                support = past_peaks[past_peaks < current_price]

                if len(resistance) > 0:
                    distance.iloc[i] = (resistance.min() - current_price) / current_price
                elif len(support) > 0:
                    distance.iloc[i] = (current_price - support.max()) / current_price

        return distance

    def _balance_peak_samples(self, features: pd.DataFrame, targets: Dict,
                              peak_mask: pd.Series) -> pd.DataFrame:
        """
        å¹³è¡¡å³°å€¼å’Œéå³°å€¼æ ·æœ¬
        """
        print("âš–ï¸ å¹³è¡¡æ ·æœ¬åˆ†å¸ƒ...")

        # åˆ†ç¦»å³°å€¼å’Œéå³°å€¼æ ·æœ¬
        peak_samples = features[peak_mask]
        non_peak_samples = features[~peak_mask]

        # ç­–ç•¥1ï¼šå¯¹éå³°å€¼æ ·æœ¬è¿›è¡Œæ¬ é‡‡æ ·
        n_peak = len(peak_samples)
        n_select = min(n_peak * 5, len(non_peak_samples))  # æœ€å¤š5:1çš„æ¯”ä¾‹

        # æ™ºèƒ½é‡‡æ ·ï¼šé€‰æ‹©æ¥è¿‘å³°å€¼çš„æ ·æœ¬
        selected_non_peak = self._smart_undersample(non_peak_samples, n_select)

        # åˆå¹¶æ•°æ®
        balanced_features = pd.concat([peak_samples, selected_non_peak])

        print(f"  å³°å€¼æ ·æœ¬: {len(peak_samples)}")
        print(f"  éå³°å€¼æ ·æœ¬: {len(selected_non_peak)}")
        print(f"  æ€»æ ·æœ¬: {len(balanced_features)}")

        return balanced_features

    def _smart_undersample(self, non_peak_samples: pd.DataFrame, n_select: int) -> pd.DataFrame:
        """
        æ™ºèƒ½æ¬ é‡‡æ ·ï¼šä¼˜å…ˆé€‰æ‹©æ¥è¿‘å³°å€¼çš„æ ·æœ¬
        """
        if len(non_peak_samples) <= n_select:
            return non_peak_samples

        # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„"å³°å€¼ç›¸ä¼¼åº¦"åˆ†æ•°
        scores = pd.Series(0.0, index=non_peak_samples.index)

        # åŸºäºå¤šä¸ªæŒ‡æ ‡è®¡ç®—ç›¸ä¼¼åº¦
        if 'rsi_14' in non_peak_samples.columns:
            rsi_extreme = np.abs(non_peak_samples['rsi_14'] - 50) / 50
            scores += rsi_extreme * 0.3

        if 'bb_position' in non_peak_samples.columns:
            bb_extreme = np.abs(non_peak_samples['bb_position'] - 0.5) * 2
            scores += bb_extreme * 0.3

        if 'atr_percentile' in non_peak_samples.columns:
            scores += non_peak_samples['atr_percentile'] * 0.2

        if 'volume_spike' in non_peak_samples.columns:
            scores += non_peak_samples['volume_spike'] * 0.2

        # é€‰æ‹©åˆ†æ•°æœ€é«˜çš„æ ·æœ¬
        top_indices = scores.nlargest(n_select).index

        return non_peak_samples.loc[top_indices]

    def train_ensemble_model(self, X: pd.DataFrame, y: pd.Series,
                             feature_cols: List[str]) -> Dict:
        """
        è®­ç»ƒé›†æˆæ¨¡å‹
        """
        print("ğŸ¤– è®­ç»ƒå³°å€¼æ„ŸçŸ¥é›†æˆæ¨¡å‹...")

        # æ•°æ®é¢„å¤„ç†
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X[feature_cols].fillna(0))

        # 1. XGBoostæ¨¡å‹
        xgb_model = xgb.XGBClassifier(
            n_estimators=200,
            max_depth=6,
            learning_rate=0.01,
            subsample=0.8,
            colsample_bytree=0.8,
            scale_pos_weight=len(y[y == 0]) / len(y[y == 1]) if len(y[y == 1]) > 0 else 1,
            random_state=42
        )

        # 2. LightGBMæ¨¡å‹
        lgb_model = lgb.LGBMClassifier(
            n_estimators=200,
            max_depth=6,
            learning_rate=0.01,
            subsample=0.8,
            colsample_bytree=0.8,
            is_unbalance=True,
            random_state=42
        )

        # 3. éšæœºæ£®æ—ï¼ˆä¸“æ³¨äºå³°å€¼ç‰¹å¾ï¼‰
        rf_model = RandomForestClassifier(
            n_estimators=300,
            max_depth=8,
            min_samples_split=10,
            min_samples_leaf=5,
            max_features='sqrt',
            class_weight='balanced',
            random_state=42
        )

        # 4. æ¢¯åº¦æå‡
        gb_model = GradientBoostingClassifier(
            n_estimators=150,
            max_depth=5,
            learning_rate=0.01,
            subsample=0.7,
            random_state=42
        )

        # 5. é›†æˆæŠ•ç¥¨åˆ†ç±»å™¨
        ensemble = VotingClassifier(
            estimators=[
                ('xgb', xgb_model),
                ('lgb', lgb_model),
                ('rf', rf_model),
                ('gb', gb_model)
            ],
            voting='soft',
            weights=[2, 2, 1, 1]  # XGBoostå’ŒLightGBMæƒé‡æ›´é«˜
        )

        # æ—¶åºäº¤å‰éªŒè¯
        tscv = TimeSeriesSplit(n_splits=5)

        # è®­ç»ƒå’Œè¯„ä¼°
        cv_scores = cross_val_score(ensemble, X_scaled, y, cv=tscv, scoring='roc_auc')
        print(f"  äº¤å‰éªŒè¯AUC: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")

        # è®­ç»ƒæœ€ç»ˆæ¨¡å‹
        ensemble.fit(X_scaled, y)

        # æå–ç‰¹å¾é‡è¦æ€§ï¼ˆä»éšæœºæ£®æ—ï¼‰
        rf_model.fit(X_scaled, y)
        feature_importance = pd.DataFrame({
            'feature': feature_cols,
            'importance': rf_model.feature_importances_
        }).sort_values('importance', ascending=False)

        print("\nğŸ“Š å³°å€¼ç‰¹å¾é‡è¦æ€§ Top 10:")
        for _, row in feature_importance.head(10).iterrows():
            print(f"  {row['feature']}: {row['importance']:.4f}")

        return {
            'ensemble': ensemble,
            'scaler': scaler,
            'feature_importance': feature_importance,
            'cv_scores': cv_scores
        }

    def detect_peak_patterns(self, X: pd.DataFrame, model: Dict) -> pd.DataFrame:
        """
        æ£€æµ‹å³°å€¼æ¨¡å¼
        """
        ensemble = model['ensemble']
        scaler = model['scaler']

        # é¢„æµ‹
        X_scaled = scaler.transform(X.fillna(0))

        # è·å–æ¦‚ç‡é¢„æµ‹
        probabilities = ensemble.predict_proba(X_scaled)

        # ç”Ÿæˆä¿¡å·
        signals = pd.DataFrame(index=X.index)

        # å³°å€¼æ¦‚ç‡
        signals['peak_probability'] = probabilities[:, 1] if probabilities.shape[1] > 1 else probabilities[:, 0]

        # å³°å€¼å¼ºåº¦åˆ†çº§
        signals['peak_signal'] = pd.cut(
            signals['peak_probability'],
            bins=[0, 0.3, 0.5, 0.7, 0.9, 1.0],
            labels=['NO_PEAK', 'WEAK_PEAK', 'MODERATE_PEAK', 'STRONG_PEAK', 'EXTREME_PEAK']
        )

        # ä½¿ç”¨Isolation Forestæ£€æµ‹å¼‚å¸¸å³°å€¼
        iso_forest = IsolationForest(
            contamination=0.05,
            random_state=42
        )
        anomaly_scores = iso_forest.fit_predict(X_scaled)
        signals['is_anomaly_peak'] = anomaly_scores == -1

        return signals